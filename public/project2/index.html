<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Charles Giacona" />
    <meta name="description" content="Describe your website">
    <link rel="shortcut icon" type="image/x-icon" href="/img/favicon.ico">
    <title>Project 2</title>
    <meta name="generator" content="Hugo 0.70.0" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="/css/main.css" />
    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,400,200bold,400old" />
    
    <!--[if lt IE 9]>
			<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
			<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
		<![endif]-->

    
  </head>

  <body>
    <div id="wrap">

      
      <nav class="navbar navbar-default">
  <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="/"><i class="fa fa-home"></i></a>
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <div class="navbar-collapse collapse" id="navbar">
      <ul class="nav navbar-nav navbar-right">
      
        
        <li><a href="/blog/">BLOG</a></li>
        
        <li><a href="/projects/">PROJECTS</a></li>
        
        <li><a href="/resume.pdf">RESUME</a></li>
        
      
      </ul>
    </div>
  </div>
</nav>

      
      <div class="container">
        <div class="blog-post">
          <h3>
            <strong><a href="/project2/">Project 2</a></strong>
          </h3>
        </div>
        <div class="blog-title">
          <h4>
          April 21, 2020
            &nbsp;&nbsp;
            
          </h4>
        </div>
        <div class="panel panel-default">
          <div class="panel-body">
            <div class="blogpost">
              


<h4>Introduction:</h4>
<p>I decided to analyze the dataset ‘Salaries’ from the package ‘carData’ for project 2. ‘Salaries’ contains 397 observations with 6 variables: ‘rank’ (AssocProf, AsstProf, and Prof), ‘discipline’ (A: “theoretical” departments/B: “applied” departments), ‘yrs.since.phd’, ‘yrs.service’, ‘sex’ (Female or Male), and ‘salary’ (nine-month salary US$). The data was initially collected to survey for pay inequalities among male and female faculty at an unnamed university.</p>
<h4>Part 1</h4>
<pre class="r"><code>data(Salaries)
glimpse(Salaries)</code></pre>
<pre><code>## Rows: 397
## Columns: 6
## $ rank &lt;fct&gt; Prof, Prof, AsstProf, Prof, Prof,
AssocProf, Prof, Prof, Prof, Prof, Asso...
## $ discipline &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B,
B, B, B, B, B, A, A, A, A, A, A, A, A...
## $ yrs.since.phd &lt;int&gt; 19, 20, 4, 45, 40, 6, 30, 45, 21,
18, 12, 7, 1, 2, 20, 12, 19, 38, 37, 39...
## $ yrs.service &lt;int&gt; 18, 16, 3, 39, 41, 6, 23, 45, 20,
18, 8, 2, 1, 0, 18, 3, 20, 34, 23, 36, ...
## $ sex &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male,
Male, Male, Female, Male, Male,...
## $ salary &lt;int&gt; 139750, 173200, 79750, 115000, 141500,
97000, 175000, 147765, 119250, 129...</code></pre>
<pre class="r"><code>man1&lt;-manova(cbind(yrs.since.phd,yrs.service,salary)~sex, data=Salaries) 
summary(man1)</code></pre>
<pre><code>## Df Pillai approx F num Df den Df Pr(&gt;F)
## sex 1 0.032223 4.3618 3 393 0.004884 **
## Residuals 395
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>summary.aov(man1)</code></pre>
<pre><code>## Response yrs.since.phd :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## sex 1 1456 1455.91 8.9424 0.002961 **
## Residuals 395 64310 162.81
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response yrs.service :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## sex 1 1583 1583.27 9.5622 0.002127 **
## Residuals 395 65403 165.58
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response salary :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## sex 1 6.9800e+09 6980014930 7.7377 0.005667 **
## Residuals 395 3.5632e+11 902077538
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>Salaries%&gt;%group_by(sex)%&gt;%
  summarize(mean(yrs.since.phd),mean(yrs.service),mean(salary))</code></pre>
<pre><code>## # A tibble: 2 x 4
## sex `mean(yrs.since.phd)` `mean(yrs.service)`
`mean(salary)`
## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 Female 16.5 11.6 101002.
## 2 Male 22.9 18.3 115090.</code></pre>
<p>There were 4 hypotheses tests conducted (1 MANOVA, 3 ANOVA). Therefore, the Bonferroni correction for type 1 error is a=(0.05/4)=0.0125. Significant differences were found among males and females for at least one of the dependent variables (Pillai= 0.032223 F(3,393) p=.0049). Univariate ANOVAs for each dependent variable were then conducted as follow-up tests to the MANOVA, using the Bonferroni method for controlling Type I error rates for multiple comparisons. The univariate ANOVAs for years since phd, yrs of service, and salary were also significant, F(1,395) p=0.002961, F(1,395) p=0.002127, F(1,395) p=0.005667 respectively. Assumptions of these tests include multivariate normality, homogeneity of groups, and no univariate or multivariate outliers present.</p>
<h4>Part 2</h4>
<pre class="r"><code>Salaries%&gt;%count(discipline)</code></pre>
<pre><code>## # A tibble: 2 x 2
##   discipline     n
##   &lt;fct&gt;      &lt;int&gt;
## 1 A            181
## 2 B            216</code></pre>
<pre class="r"><code>A&lt;-subset(Salaries, discipline==&quot;A&quot;, select=salary)
A&lt;-unlist(A)
B&lt;-subset(Salaries, discipline==&quot;B&quot;, select=salary)
B&lt;-unlist(B)
discipline_salary&lt;-data.frame(discipline=c(rep(&quot;A&quot;,181),
                                           rep(&quot;B&quot;,216)),salary=c(A,B))
#Mean diff in salary across disipline
discipline_salary%&gt;%group_by(discipline)%&gt;%
  summarize(means=mean(salary))%&gt;%
  summarize(`mean_diff:`=diff(means))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   `mean_diff:`
##          &lt;dbl&gt;
## 1        9480.</code></pre>
<pre class="r"><code>#randomization 
rand_dist&lt;-vector()
for(i in 1:5000){
new&lt;-data.frame(salary=sample(discipline_salary$salary),
                discipline=discipline_salary$discipline) 
rand_dist[i]&lt;-mean(new[new$discipline==&quot;A&quot;,]$salary)-
              mean(new[new$discipline==&quot;B&quot;,]$salary)}

#histogram of random dist with calculated mean diff plotted
hist(rand_dist,main=&quot;Null Random Distribution of Discipline vs. Salary&quot;,ylab=&quot;&quot;); 
abline(v = 9480.264,col=&quot;red&quot;)</code></pre>
<p><img src="/project2_files/figure-html/Randomization%20Test-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>mean(rand_dist&lt; -9480.264 | rand_dist&gt; 9480.264)</code></pre>
<pre><code>## [1] 0.0012</code></pre>
<pre class="r"><code>t.test(data=discipline_salary,salary~discipline)</code></pre>
<pre><code>##
## Welch Two Sample t-test
##
## data: salary by discipline
## t = -3.1306, df = 377.83, p-value = 0.00188
## alternative hypothesis: true difference in means is not
equal to 0
## 95 percent confidence interval:
## -15434.549 -3525.978
## sample estimates:
## mean in group A mean in group B
## 108548.4 118028.7</code></pre>
<div id="null-hypotheses-mean-salary-is-the-same-for-discipline-a-vs.-disciplineb." class="section level4">
<p>Null hypotheses: Mean salary is the same for discipline A vs. disciplineB.</p>
<p>Alternative hypotheses: Mean salary is not the same for discipline A vs. discipline B.
A two tailed t-test comparing the mean difference in salaries from the two groups to the randomly generated mean yeailded a p-value of 0.0018. Therefore, the null hypotheses is rejected.</p>
<h4>Part 3</h4>
<pre class="r"><code>#Multiple Regression
Salaries1&lt;-Salaries
Salaries1$yrs.service_c &lt;- Salaries1$yrs.service - 
  mean(Salaries1$yrs.service) 
fit&lt;-lm(salary ~ yrs.service_c * discipline, data=Salaries1) 
summary(fit)</code></pre>
<pre><code>##
## Call:
## lm(formula = salary ~ yrs.service_c * discipline, data =
Salaries1)
##
## Residuals:
## Min 1Q Median 3Q Max
## -86326 -19779 -4999 16091 102274
##
## Coefficients:
## Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 107317.9 2076.8 51.674 &lt; 2e-16 ***
## yrs.service_c 526.8 150.1 3.510 0.000499 ***
## disciplineB 13102.5 2813.7 4.657 4.4e-06 ***
## yrs.service_c:disciplineB 695.2 215.9 3.220 0.001388 **
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Residual standard error: 27540 on 393 degrees of freedom
## Multiple R-squared: 0.1795, Adjusted R-squared: 0.1733
## F-statistic: 28.67 on 3 and 393 DF, p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>#Plot of regression
ggplot(Salaries1, aes(x=yrs.service_c, y=salary,
                      color=discipline))+ geom_point()+
  geom_smooth(method=&quot;lm&quot;,formula=y~1,se=F,
              fullrange=T,aes(color=discipline))+
  ggtitle(&quot;t-test&quot;)+xlab(&quot;Years of Service&quot;)+
  ylab(&quot;Salary&quot;)</code></pre>
<p><img src="/project2_files/figure-html/Linear%20Regression-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>resids&lt;-fit$residuals
fitvals&lt;-fit$fitted.values

#Test graph of linearity/ normality
ggplot()+geom_qq(aes(sample=resids))+
  geom_qq_line(aes(sample=resids), color=&#39;red&#39;)</code></pre>
<p><img src="/project2_files/figure-html/Linear%20Regression-2.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Formal test normality
ks.test(resids, &quot;pnorm&quot;, mean=0, sd(resids)) </code></pre>
<pre><code>## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  resids
## D = 0.079736, p-value = 0.01284
## alternative hypothesis: two-sided</code></pre>
<pre class="r"><code>#Formal test homoskedasticity
bptest(fit)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  fit
## BP = 16.114, df = 3, p-value = 0.001074</code></pre>
<pre class="r"><code>#Robust SE
coeftest(fit, vcov = vcovHC(fit))[,1:2]</code></pre>
<pre><code>##                              Estimate Std. Error
## (Intercept)               107317.9333  2215.5122
## yrs.service_c                526.8293   178.8339
## disciplineB                13102.4513  2932.4255
## yrs.service_c:disciplineB    695.1651   261.1479</code></pre>
<pre class="r"><code>#Proprtion of variance
(sum((Salaries$salary-mean(Salaries$salary))^2)-
    sum(fit$residuals^2))/sum((Salaries$salary-
                                 mean(Salaries$salary))^2)</code></pre>
<pre><code>## [1] 0.179542</code></pre>
</div>
<div id="yrs.service_c-controlling-for-discipline-for-every-1-unit-increase-in-yrs.service_c-salary-goes-up-by-526.80-on-average.-std.-error-150.1-t-value-3.510-pprt-0.000499" class="section level4">
<p>yrs.service_c: Controlling for discipline, for every 1-unit increase in ‘yrs.service_c’, salary goes up by $526.80 on average. (std. error= 150.1 , t value= 3.510, pPr(&gt;|t|)= 0.000499)</p>
</div>
<div id="disciplineb-controlling-for-yrs.service_c-salary-is-13102.50-higher-in-discipline-b-than-discipline-a.-std.-error-2813.7-t-value-4.657-pprt-0.0000044" class="section level4">
<p>disciplineB: Controlling for ‘yrs.service_c’, salary is $13102.50 higher in discipline B than discipline A. (std. error= 2813.7, t value= 4.657, pPr(&gt;|t|)= 0.0000044)</p>
</div>
<div id="yrs.service_cdisciplineb-controlling-the-slope-of-yrs.service_c-salary-is-695.20-higher-in-discipline-b-than-discipline-a.-std.-error-215.9-t-value-3.220-pprt-0.001388" class="section level4">
<p>yrs.service_c:disciplineB: Controlling the slope of ‘yrs.service_c’, salary is $695.20 higher in discipline B than discipline A. (std. error= 215.9, t value= 3.220, pPr(&gt;|t|)= 0.001388)</p>
</div>
<div id="rejecting-the-null-hypothesis-of-no-homoskedasticity-i-computed-robust-se.-the-new-std.-errors-for-the-coefficients-yrs.service-disiplineb-and-their-interaction-are-178.8339-2932.4255-and-261.1479-respectively.-these-values-are-larger-than-the-values-calculated-in-the-original-regression-because-of-the-homoskedasticity.-aditionally-0.1795-is-the-proportion-of-variation-in-the-response-variable-explained-by-the-overall-model." class="section level4">
<p>Rejecting the null hypothesis of no homoskedasticity I computed robust SE. The new Std. Errors for the coefficients yrs.service, disiplineB, and their interaction are 178.8339, 2932.4255, and 261.1479 respectively. These values are larger than the values calculated in the original regression because of the homoskedasticity. Aditionally, 0.1795 is the proportion of variation in the response variable explained by the overall model.</p>
<h4>Part 4</h4>
<pre class="r"><code>#Bootstrapped Sample
samp_distn&lt;-replicate(5000, {  
  boot_dat&lt;-Salaries1[sample(nrow(Salaries1),replace=TRUE),]  
  fit1&lt;-lm(salary ~ yrs.service_c * discipline, data=boot_dat)  
  coef(fit1) 
}) 

#Normal SE
coeftest(fit)[,1:2] </code></pre>
<pre><code>##                              Estimate Std. Error
## (Intercept)               107317.9333  2076.8304
## yrs.service_c                526.8293   150.0725
## disciplineB                13102.4513  2813.6881
## yrs.service_c:disciplineB    695.1651   215.8791</code></pre>
<pre class="r"><code>#Bootstrapped SE
samp_distn%&gt;%t%&gt;%as.data.frame%&gt;%summarize_all(sd)</code></pre>
<pre><code>## (Intercept) yrs.service_c disciplineB
yrs.service_c:disciplineB
## 1 2209.701 176.0761 2915.36 258.2098</code></pre>
<p>The Bootstrapped SE for ‘yrs.service_c’, ‘disciplineB’, and ‘yrs.service_c:disciplineB’ are 174.433, 2955.389, and 254.9534 respectively. These values are larger for all three variables compared to the normal SE’s but only larger than the robust SE for the coefficient ‘disciplineB’.</p>
<h4>Part 5</h4>
<pre class="r"><code>fit3&lt;-glm(discipline~salary+yrs.service+sex,
          data=Salaries1,family=&quot;binomial&quot;) 
coeftest(fit3)</code></pre>
<pre><code>##
## z test of coefficients:
##
## Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -1.1550e+00 4.9901e-01 -2.3146 0.02063 *
## salary 1.7800e-05 4.0830e-06 4.3595 1.304e-05 ***
## yrs.service -4.2081e-02 9.3456e-03 -4.5028 6.707e-06 ***
## sexMale 6.2184e-02 3.5063e-01 0.1774 0.85923
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>exp(coef(fit3))</code></pre>
<pre><code>## (Intercept)      salary yrs.service     sexMale 
##   0.3150515   1.0000178   0.9587918   1.0641579</code></pre>
<pre class="r"><code>#confusion matrix
prob&lt;-predict(fit3,type=&quot;response&quot;) 
pred&lt;-ifelse(prob&gt;.5,&quot;B&quot;,&quot;A&quot;) 

table(prediction=pred, truth=Salaries$discipline)%&gt;%addmargins</code></pre>
<pre><code>##           truth
## prediction   A   B Sum
##        A    80  38 118
##        B   101 178 279
##        Sum 181 216 397</code></pre>
<pre class="r"><code>(80+178)/397 #Accuracy</code></pre>
<pre><code>## [1] 0.6498741</code></pre>
<pre class="r"><code>80/118 #Sensitivity </code></pre>
<pre><code>## [1] 0.6779661</code></pre>
<pre class="r"><code>178/279 #Specificity </code></pre>
<pre><code>## [1] 0.6379928</code></pre>
<pre class="r"><code>80/181 #Precision </code></pre>
<pre><code>## [1] 0.441989</code></pre>
<pre class="r"><code>#Density plot of log-odds
Salaries1$logit&lt;-predict(fit3)
ggplot(Salaries1,aes(logit, fill=discipline))+geom_density(alpha=.3)+
  geom_vline(xintercept=0,lty=2)+
  ggtitle(&quot;Density Plot of Log-odds&quot;)</code></pre>
<p><img src="/project2_files/figure-html/Logistic%20regression%20and%20CV-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#ROC Plot
ROCplot&lt;-ggplot(Salaries)+
  geom_roc(aes(d=discipline,m=prob), n.cuts=0)+ 
  geom_segment(aes(x=0,xend=1,y=0,yend=1),lty=2)+
  ggtitle(&quot;ROC Plot&quot;)
ROCplot</code></pre>
<p><img src="/project2_files/figure-html/Logistic%20regression%20and%20CV-2.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>calc_auc(ROCplot)</code></pre>
<pre><code>##   PANEL group      AUC
## 1     1    -1 0.683318</code></pre>
<pre class="r"><code>#10 Fold CV
set.seed(1234)
k=10
data&lt;-Salaries[sample(nrow(Salaries)),] 
folds&lt;-cut(seq(1:nrow(Salaries)),breaks=k,labels=F)  
diags&lt;-NULL
for(i in 1:k){
  train1 &lt;- data[folds!=i,] 
  test &lt;- data[folds==i,] 
  truth &lt;- test$discipline  
  fit4 &lt;- glm(discipline~salary+yrs.service+sex+rank, 
             data=train1, family=&quot;binomial&quot;)
  probs1 &lt;- predict(fit4, newdata=test, type=&quot;response&quot;)
  diags&lt;-rbind(diags,class_diag(probs1,truth))
}
diags%&gt;%summarize_all(mean)</code></pre>
<pre><code>##         acc      sens      spec       ppv       auc
## 1 0.6347436 0.7493051 0.4922573 0.6361637 0.6858376</code></pre>
<p>For every one unit increase in salary the odds of predicting discipline B change by a factor of 1.0000178. For every one unit increase in years of service the odds of predicting discipline B change by a factor of 0.9587918. Controlling for salary and years of service, there is a 1.0641579 odds higher for males to be in discipline B relative to females.
From the confusion matrix, the accuracy, sensitivity, specificity, and precision of this model are 0.65, 0.68, 0.64, and 0.44 respectively. Aditionally, the AUC is equal to 0.68 and was computed using ‘calc_auc’ from the generated ROC plot. This is a “poor” trade-off between sensitivity and specificity in the model. The 10-fold CV reported that the average out-of-sample AUC, Accuracy, Sensitivity, and Recall are 0.68, 0.64, 0.49, and 0.64 respectively.</p>
<h4>Part 6</h4>
<pre class="r"><code>library(glmnet)
set.seed(1234)
y&lt;-as.matrix(Salaries$discipline) 
x&lt;-model.matrix(discipline~.,data=Salaries)[,-1] 
cv&lt;-cv.glmnet(x,y,family=&quot;binomial&quot;)
lasso&lt;-glmnet(x,y,family=&quot;binomial&quot;,lambda=cv$lambda.1se)
coef(lasso)</code></pre>
<pre><code>## 7 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                          s0
## (Intercept)   -3.769542e-01
## rankAssocProf  .           
## rankProf       .           
## yrs.since.phd -3.389210e-02
## yrs.service    .           
## sexMale        .           
## salary         1.153799e-05</code></pre>
<pre class="r"><code>#10 Fold CV
set.seed(1234)
k=10
data&lt;-Salaries[sample(nrow(Salaries)),] 
folds&lt;-cut(seq(1:nrow(Salaries)),breaks=k,labels=F)   
diags1&lt;-NULL
for(i in 1:k){
  train &lt;- data[folds!=i,] 
  test &lt;- data[folds==i,] 
  truth &lt;- test$discipline  
  fit5 &lt;- glm(discipline~ salary+yrs.since.phd, 
             data=train, family=&quot;binomial&quot;)
  probs2 &lt;- predict(fit5, newdata=test, type=&quot;response&quot;)
  diags1&lt;-rbind(diags1,class_diag(probs2,truth))
}
diags1%&gt;%summarize_all(mean)</code></pre>
<pre><code>##         acc      sens      spec     ppv       auc
## 1 0.6448718 0.7701811 0.4836199 0.64528 0.7043581</code></pre>
<p>The variables retained from the LASSO regression, predicting discipline, are ‘salary’ and ‘yrs.since.phd.’ Using these retained variables from the LASSO regression, a 10-fold CV yielded an AUC of 0.71 which is a better CV performance compared to the CV in part five which yielded an AUC of 0.68. However, ACC remained roughly the same at .063.</p>
</div>

              <hr>
              <div class="related-posts">
                <h5>Related Posts</h5>
                
              </div>
            </div>
          </div>
          <hr>
        <div class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">

    (function() {
      
      
      if (window.location.hostname == "localhost")
        return;

      var disqus_shortname = '';
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
        </div>
      </div>
      
    </div>

    
    <footer>
  <div id="footer">
    <div class="container">
      <p class="text-muted">&copy; All rights reserved. Powered by <a href="https://gohugo.io/">Hugo</a> and
      <a href="http://www.github.com/nurlansu/hugo-sustain/">sustain</a> with ♥</p>
    </div>
  </div>
</footer>
<div class="footer"></div>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="/js/docs.min.js"></script>
<script src="/js/main.js"></script>

<script src="/js/ie10-viewport-bug-workaround.js"></script>


    
  </body>
</html>
